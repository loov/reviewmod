// reviewmod.cue - Example configuration

llm: {
	provider:  "openai"
	base_url:  "http://localhost:8080/v1"
	model:     "llama3"
	max_tokens: 4096
	temperature: 0.1
}

cache: {
	dir: ".reviewmod/cache"
	enabled: true
}

output: {
	json:     "reviewmod-report.json"
	markdown: "reviewmod-report.md"
}

analyses: [
	{name: "summary", prompt: "prompts/summary.txt"},
	{name: "security", prompt: "prompts/security.txt", include_security_properties: true},
	{name: "errors", prompt: "prompts/errors.txt"},
	{name: "cleanliness", prompt: "prompts/cleanliness.txt"},
]
